"""
演示获取PySpark的执行环境入库对象: SparkContext
并通过SparkContext对象获取当前PySpark的版本
"""
 
# 导包
from pyspark import SparkConf,SparkContext


# 创建SparkConf类对象中
conf = SparkConf().setSparkHome("local[*]").setAppName("test_spqrk_app")


#基SparkConf类对象创LSparkContext对象
sc = SparkContext(conf = conf)


arr = [1,2,3,4,66,778]
rdd1 = sc.parallelize(arr)


rdd1.collect()


rdd1


# 为每个元素执行的函数
def func(element):
    return element * 10


# 应用 map 操作，将每个元素乘以 10
rdd2 = rdd1.map(func)


rdd2.collect()


#打印PySpark的运行版本
print(sc.version)


#停止SparkContext对象的运行(停止PySpark程序)
sc.stop()


from pyspark import sql


s = sql.SparkSession.builder.enableHiveSupport().getOrCreate()


def get(x):
    print(x)

get(123)



